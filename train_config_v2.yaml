# Training config v2 - Contrastive Loss Trainer
# Curriculum mining: random → semi-hard → hard

training:
  max_epochs: 100
  patience: 15  # Early stopping patience

optimizer:
  learning_rate: 5.0e-4
  weight_decay: 1.0e-5
  lr_scheduler_factor: 0.5
  lr_scheduler_patience: 5

loss:
  margin: 0.5  # Contrastive loss margin

# Evaluation parameters
evaluation:
  sample_size: 5000    # Number of samples from validation set for faster evaluation

# Curriculum mining: 3 fasi
mining:
  # Negative mining strategy
  random_epochs: 5      # Epoch 1-5: random mining
  semihard_epochs: 20     # Epoch 6-20: semi-hard mining
  hardmining_epochs: 100  # Epoch 21-100: hard mining

  # Positive mining strategy (progressive)
  positive_random_epochs: 10   # Epoch 1-10: random positive (one random per anchor)
  positive_hard_epochs: 100    # Epoch 11-100: hard positive (hardest per anchor)

# PK sampling per batch
batch:
  num_patients: 32  # P: numero pazienti per batch
  num_ecg: 8        # K: numero ECG per paziente
  # Batch size = 64 * 4 = 256

encoder:
  input_dim: 13
  hidden_dims: [32, 64, 128]
  embedding_dim: 64
  dropout: 0.2
  normalize: true

data:
  # Path relativi rispetto alla directory del progetto
  train_csv: data/ECG/train.csv
  val_csv: data/ECG/val.csv
  test_csv : data/ECG/test.csv

device: cuda
